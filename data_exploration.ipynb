{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_zip_folder(file_type, zip_folder_path, new_folder_path):\n",
    "    with ZipFile(zip_folder_path, 'r') as zip_obj:\n",
    "       # Get a list of all archived file names from the zip\n",
    "       list_of_file_names = zip_obj.namelist()\n",
    "       # Iterate over the file names\n",
    "       for file_name in list_of_file_names:\n",
    "           # Check filename endswith csv\n",
    "           if file_name.endswith(f'.{file_type}'):\n",
    "               # Extract a single file from zip\n",
    "               zip_obj.extract(file_name, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_from_zip_folder('csv', 'data/Anio201810.zip', 'csv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>ESTACION</th>\n",
       "      <th>MAGNITUD</th>\n",
       "      <th>PUNTO_MUESTREO</th>\n",
       "      <th>ANO</th>\n",
       "      <th>MES</th>\n",
       "      <th>DIA</th>\n",
       "      <th>H01</th>\n",
       "      <th>V01</th>\n",
       "      <th>...</th>\n",
       "      <th>H20</th>\n",
       "      <th>V20</th>\n",
       "      <th>H21</th>\n",
       "      <th>V21</th>\n",
       "      <th>H22</th>\n",
       "      <th>V22</th>\n",
       "      <th>H23</th>\n",
       "      <th>V23</th>\n",
       "      <th>H24</th>\n",
       "      <th>V24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28079004_1_38</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V</td>\n",
       "      <td>4.0</td>\n",
       "      <td>V</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28079004_1_38</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28079004_1_38</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28079004_1_38</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28079004_1_38</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V</td>\n",
       "      <td>4.0</td>\n",
       "      <td>V</td>\n",
       "      <td>5.0</td>\n",
       "      <td>V</td>\n",
       "      <td>4.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROVINCIA  MUNICIPIO  ESTACION  MAGNITUD PUNTO_MUESTREO   ANO  MES  DIA  \\\n",
       "0         28         79         4         1  28079004_1_38  2018    4    1   \n",
       "1         28         79         4         1  28079004_1_38  2018    4    2   \n",
       "2         28         79         4         1  28079004_1_38  2018    4    3   \n",
       "3         28         79         4         1  28079004_1_38  2018    4    4   \n",
       "4         28         79         4         1  28079004_1_38  2018    4    5   \n",
       "\n",
       "   H01 V01  ...  H20 V20  H21 V21  H22 V22  H23 V23  H24 V24  \n",
       "0  2.0   V  ...  2.0   V  2.0   V  3.0   V  4.0   V  3.0   V  \n",
       "1  2.0   V  ...  2.0   V  2.0   V  2.0   V  2.0   V  2.0   V  \n",
       "2  2.0   V  ...  2.0   V  2.0   V  2.0   V  2.0   V  2.0   V  \n",
       "3  2.0   V  ...  2.0   V  2.0   V  2.0   V  2.0   V  2.0   V  \n",
       "4  2.0   V  ...  2.0   V  2.0   V  4.0   V  5.0   V  4.0   V  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv_data/abr_mo18.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4490, 56)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_days(dataframe, sample_spot):\n",
    "    '''This function takes the monthly dataframe and checks if there are missing days\n",
    "    for a specific sample spot. If so, it appends a row to the original dataframe with\n",
    "    the info of that day and the validation columns set to N (non validated), to know that\n",
    "    info is not correct (we will correct it later).\n",
    "    '''\n",
    "    year = dataframe.loc[0, 'ANO']\n",
    "    month = dataframe.loc[0, 'MES']\n",
    "    # First we have to know how many days a specific month has:\n",
    "    _, number_days_month = calendar.monthrange(year, month)\n",
    "    \n",
    "    # We create a df with just the info of one spot\n",
    "    sample_spot_df = dataframe[dataframe['PUNTO_MUESTREO'] == sample_spot].reset_index()\n",
    "    \n",
    "    # We create a list with all the days of that month\n",
    "    list_of_days_of_the_month = list(range(1, number_days_month + 1))\n",
    "    \n",
    "    # We check if all that days are contained in the spot df\n",
    "    isin_df = sample_spot_df['DIA'].isin(list_of_days_of_the_month)\n",
    "    isin_df.index = list_of_days_of_the_month\n",
    "    print(isin_df)\n",
    "    \n",
    "    # Now, if a day is not included, we append a row with its data to the original df\n",
    "    for day, isin in isin_df.iteritems():\n",
    "        if isin == False:\n",
    "            print(f'Day {day}-{month}-{year} missing')\n",
    "            # We take the first row of the df, but we change the day and the validation columns to 'N'\n",
    "            row_to_append = [sample_spot_df.loc[0, column] for column in sample_spot_df.columns]\n",
    "            row_to_append[8] = day\n",
    "            for i, e in enumerate(row_to_append):\n",
    "                if e == 'V':\n",
    "                    row_to_append[i] = 'N'\n",
    "            print(row_to_append)\n",
    "            # We append the row\n",
    "            dataframe = dataframe.append(pd.Series(row_to_append, index=sample_spot_df.columns), ignore_index=True)\n",
    "            \n",
    "    print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4490, 56)\n",
      "   index  PROVINCIA  MUNICIPIO  ESTACION  MAGNITUD PUNTO_MUESTREO   ANO  MES  \\\n",
      "0     90         28         79         4         8   28079004_8_8  2018    4   \n",
      "1     91         28         79         4         8   28079004_8_8  2018    4   \n",
      "2     92         28         79         4         8   28079004_8_8  2018    4   \n",
      "3     93         28         79         4         8   28079004_8_8  2018    4   \n",
      "4     94         28         79         4         8   28079004_8_8  2018    4   \n",
      "\n",
      "   DIA   H01  ...   H20  V20   H21  V21   H22  V22   H23  V23   H24  V24  \n",
      "0    1  21.0  ...  46.0    V  57.0    V  82.0    V  85.0    V  77.0    V  \n",
      "1    2  67.0  ...  22.0    V  26.0    V  38.0    V  15.0    V  15.0    V  \n",
      "2    3  14.0  ...  23.0    V  27.0    V  30.0    V  23.0    V  20.0    V  \n",
      "3    4   8.0  ...  14.0    V  19.0    V  22.0    V  28.0    V  23.0    V  \n",
      "4    5  20.0  ...  45.0    V  68.0    V  99.0    V  93.0    V  84.0    V  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "1     True\n",
      "2     True\n",
      "3     True\n",
      "4     True\n",
      "5     True\n",
      "6     True\n",
      "7     True\n",
      "8     True\n",
      "9     True\n",
      "10    True\n",
      "11    True\n",
      "12    True\n",
      "13    True\n",
      "14    True\n",
      "15    True\n",
      "16    True\n",
      "17    True\n",
      "18    True\n",
      "19    True\n",
      "20    True\n",
      "21    True\n",
      "22    True\n",
      "23    True\n",
      "24    True\n",
      "25    True\n",
      "26    True\n",
      "27    True\n",
      "28    True\n",
      "29    True\n",
      "30    True\n",
      "Name: DIA, dtype: bool\n",
      "(day: 1, isin: True\n",
      "Day 1-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 1, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 2, isin: True\n",
      "Day 2-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 2, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 3, isin: True\n",
      "Day 3-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 3, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 4, isin: True\n",
      "Day 4-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 4, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 5, isin: True\n",
      "Day 5-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 5, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 6, isin: True\n",
      "Day 6-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 6, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 7, isin: True\n",
      "Day 7-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 7, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 8, isin: True\n",
      "Day 8-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 8, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 9, isin: True\n",
      "Day 9-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 9, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 10, isin: True\n",
      "Day 10-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 10, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 11, isin: True\n",
      "Day 11-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 11, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 12, isin: True\n",
      "Day 12-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 12, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 13, isin: True\n",
      "Day 13-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 13, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 14, isin: True\n",
      "Day 14-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 14, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 15, isin: True\n",
      "Day 15-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 15, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 16, isin: True\n",
      "Day 16-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 16, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 17, isin: True\n",
      "Day 17-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 17, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 18, isin: True\n",
      "Day 18-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 18, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 19, isin: True\n",
      "Day 19-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 19, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 20, isin: True\n",
      "Day 20-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 20, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 21, isin: True\n",
      "Day 21-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 21, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 22, isin: True\n",
      "Day 22-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 22, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 23, isin: True\n",
      "Day 23-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 23, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(day: 24, isin: True\n",
      "Day 24-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 24, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 25, isin: True\n",
      "Day 25-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 25, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 26, isin: True\n",
      "Day 26-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 26, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 27, isin: True\n",
      "Day 27-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 27, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 28, isin: True\n",
      "Day 28-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 28, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 29, isin: True\n",
      "Day 29-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 29, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(day: 30, isin: True\n",
      "Day 30-4-2018 missing\n",
      "[90, 28, 79, 4, 8, '28079004_8_8', 2018, 4, 30, 21.0, 'N', 19.0, 'N', 17.0, 'N', 24.0, 'N', 17.0, 'N', 35.0, 'N', 40.0, 'N', 36.0, 'N', 34.0, 'N', 35.0, 'N', 44.0, 'N', 44.0, 'N', 38.0, 'N', 26.0, 'N', 29.0, 'N', 22.0, 'N', 17.0, 'N', 24.0, 'N', 32.0, 'N', 46.0, 'N', 57.0, 'N', 82.0, 'N', 85.0, 'N', 77.0, 'N']\n",
      "(4520, 57)\n"
     ]
    }
   ],
   "source": [
    "check_days(data, '28079004_8_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_dataframe(dataframe, cols_to_drop, cols_remain):\n",
    "    '''This function applies the pandas stack method to make data that is\n",
    "    spread in columns collapse in a single column.\n",
    "    First drops the columns that would not let the stack work properly.\n",
    "    Then sets the columns that do not have to be stacked as the index.\n",
    "    Applies stack method. Finally, resets index.\n",
    "    '''\n",
    "    dataframe = dataframe.drop(columns=cols_to_drop)\n",
    "    dataframe = dataframe.set_index(cols_remain)\n",
    "    dataframe = dataframe.stack().reset_index()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_col_to_df(df1, df2):\n",
    "    ''' Adds the last column from a dataframe to another dataframe with the same number of rows'''\n",
    "    df1['new_col'] = df2.iloc[:,-1]\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reshaped_df(dataframe):\n",
    "    '''Gets a df, keeps just the NO2 info, splits it into 2 dataframes,\n",
    "    each of them with one of the columns that we want to stack,\n",
    "    joins them into a single dataframe, renames columns and formats HORA column.\n",
    "    The result is a much easier to use dataframe'''\n",
    "    \n",
    "    print(f'Dataframe shape: {dataframe.shape}')\n",
    "    \n",
    "    print('Keeping just NO2 data')\n",
    "    dataframe = dataframe[dataframe['MAGNITUD'] == 8].drop(columns=['MAGNITUD'])\n",
    "    cols_dimensiones = ['PROVINCIA', 'MUNICIPIO', 'ESTACION', 'PUNTO_MUESTREO', 'ANO', 'MES', 'DIA']\n",
    "    \n",
    "    print('Stacking dataframes')\n",
    "    df_h = get_stacked_dataframe(\n",
    "        dataframe,\n",
    "        cols_remain=cols_dimensiones,\n",
    "        cols_to_drop=[col for col in list(dataframe.columns) if col[0] == 'V']\n",
    "    )\n",
    "    \n",
    "    df_v = get_stacked_dataframe(\n",
    "        dataframe,\n",
    "        cols_remain=cols_dimensiones,\n",
    "        cols_to_drop=[col for col in list(dataframe.columns) if col[0] == 'H']\n",
    "    )\n",
    "    \n",
    "    print('Joining dataframes')\n",
    "    final_df = add_last_col_to_df(df_h, df_v)\n",
    "    \n",
    "    print('Renaming columns')\n",
    "    final_df = final_df.rename(columns={'level_7': 'HORA', 0: 'NIVEL_NO2', 'new_col': 'VALIDADO'})\n",
    "    \n",
    "    print('Formatting HORA column')\n",
    "    final_df['HORA'] = final_df['HORA'].apply(lambda x: int(x[-2:]))\n",
    "    \n",
    "    print('Sorting dataframe by year, month and day')\n",
    "    \n",
    "    print(f'Final dataframe shape: {final_df.shape}')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (4490, 56)\n",
      "Keeping just NO2 data\n",
      "Stacking dataframes\n",
      "Joining dataframes\n",
      "Renaming columns\n",
      "Formatting HORA column\n",
      "Sorting dataframe by year, month and day\n",
      "Final dataframe shape: (17256, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>ESTACION</th>\n",
       "      <th>PUNTO_MUESTREO</th>\n",
       "      <th>ANO</th>\n",
       "      <th>MES</th>\n",
       "      <th>DIA</th>\n",
       "      <th>HORA</th>\n",
       "      <th>NIVEL_NO2</th>\n",
       "      <th>VALIDADO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>28079004_8_8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>28079004_8_8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>28079004_8_8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>28079004_8_8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "      <td>28079004_8_8</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROVINCIA  MUNICIPIO  ESTACION PUNTO_MUESTREO   ANO  MES  DIA  HORA  \\\n",
       "0         28         79         4   28079004_8_8  2018    4    1     1   \n",
       "1         28         79         4   28079004_8_8  2018    4    1     2   \n",
       "2         28         79         4   28079004_8_8  2018    4    1     3   \n",
       "3         28         79         4   28079004_8_8  2018    4    1     4   \n",
       "4         28         79         4   28079004_8_8  2018    4    1     5   \n",
       "\n",
       "   NIVEL_NO2 VALIDADO  \n",
       "0       21.0        V  \n",
       "1       19.0        V  \n",
       "2       17.0        V  \n",
       "3       24.0        V  \n",
       "4       17.0        V  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_df = get_reshaped_df(data)\n",
    "reshaped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('data/Anio201810.zip', 'r') as zipObj:\n",
    "   # Get a list of all archived file names from the zip\n",
    "   listOfFileNames = zipObj.namelist()\n",
    "   # Iterate over the file names\n",
    "   for fileName in listOfFileNames:\n",
    "       # Check filename endswith csv\n",
    "       if fileName.endswith('.csv'):\n",
    "           # Extract a single file from zip\n",
    "           zipObj.extract(fileName, 'csv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas: para rellenar los valores que faltan\n",
    "# comprobar si faltan datos de días e incluirlos\n",
    "# para valores no validados:\n",
    "    # media del nivel de NO2 de ese punto de muestreo en cada hora\n",
    "    # append de todos los df, ordenar por pm, año, mes y hora y rellenar con la media de los valores contíguos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ver si faltan días e incluírlos\n",
    "\n",
    "year = reshaped_df.loc[0, 'ANO']\n",
    "month = reshaped_df.loc[0, 'MES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, number_days_month = calendar.monthrange(year, month)\n",
    "number_days_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_spot = '28079004_8_8'\n",
    "sample_spot_df = reshaped_df[reshaped_df['PUNTO_MUESTREO'] == sample_spot]\n",
    "sample_spot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
